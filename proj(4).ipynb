{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c538007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OSGM-H vs GD, Adam, AdaGrad (with MNIST)\n",
      "======================================================================\n",
      "Loading MNIST dataset...\n",
      "  Full binary subset (0 vs 1): 14780 samples\n",
      "  Using 2000 samples\n",
      "  Features: 784 (28x28 pixels)\n",
      "  Class distribution: 961 zeros, 1039 ones\n",
      "\n",
      "==================================================\n",
      "Problem: Quadratic $\\|Ax-b\\|^2$\n",
      "==================================================\n",
      "  Running GD...\n",
      "    Iter 0: f = 7.768161e-01, time = 0.00s\n",
      "    Iter 500: f = 1.564778e-02, time = 0.01s\n",
      "    Iter 1000: f = 1.120617e-02, time = 0.01s\n",
      "    Iter 1500: f = 8.951422e-03, time = 0.02s\n",
      "  Running Adam...\n",
      "    Iter 500: f = 2.148593e-03, time = 0.01s\n",
      "    Iter 1000: f = 1.864089e-03, time = 0.02s\n",
      "    Iter 1500: f = 1.836492e-03, time = 0.03s\n",
      "    Iter 2000: f = 1.833833e-03, time = 0.04s\n",
      "  Running AdaGrad...\n",
      "    Iter 0: f = 1.325292e+00, time = 0.00s\n",
      "    Iter 500: f = 3.995661e-03, time = 0.01s\n",
      "    Iter 1000: f = 2.520077e-03, time = 0.01s\n",
      "    Iter 1500: f = 2.120709e-03, time = 0.02s\n",
      "  Running OSGM-H...\n",
      "    Iter 0: f = 1.039531e+00, α = 0.50, time = 0.00s\n",
      "    Iter 500: f = 4.159829e-03, α = 1.81, time = 0.01s\n",
      "    Iter 1000: f = 2.775500e-03, α = 1.33, time = 0.02s\n",
      "    Iter 1500: f = 2.346565e-03, α = 1.15, time = 0.03s\n",
      "\n",
      "==================================================\n",
      "Problem: Quartic $\\|Ax-b\\|^4$\n",
      "==================================================\n",
      "  Running GD...\n",
      "    Iter 0: f = 1.133792e+00, time = 0.00s\n",
      "    Iter 500: f = 9.587055e-02, time = 0.01s\n",
      "    Iter 1000: f = 4.475824e-02, time = 0.01s\n",
      "    Iter 1500: f = 2.907672e-02, time = 0.02s\n",
      "  Running Adam...\n",
      "    Iter 500: f = 9.978054e-04, time = 0.01s\n",
      "    Iter 1000: f = 6.800542e-04, time = 0.02s\n",
      "    Iter 1500: f = 5.010974e-04, time = 0.04s\n",
      "    Iter 2000: f = 3.702605e-04, time = 0.05s\n",
      "  Running AdaGrad...\n",
      "    Iter 0: f = 3.205835e-01, time = 0.00s\n",
      "    Iter 500: f = 9.154267e-04, time = 0.01s\n",
      "    Iter 1000: f = 7.230793e-04, time = 0.02s\n",
      "    Iter 1500: f = 6.231078e-04, time = 0.02s\n",
      "  Running OSGM-H...\n",
      "    Iter 0: f = 1.147166e+00, α = 0.10, time = 0.00s\n",
      "    Iter 500: f = 4.399510e-05, α = 49.10, time = 0.01s\n",
      "    Iter 1000: f = 1.427922e-05, α = 98.97, time = 0.03s\n",
      "    Iter 1500: f = 7.982385e-06, α = 148.89, time = 0.04s\n",
      "\n",
      "==================================================\n",
      "Problem: $\\|Ax-b\\|^6$\n",
      "==================================================\n",
      "  Running GD...\n",
      "    Iter 0: f = 1.230696e+00, time = 0.00s\n",
      "    Iter 500: f = 1.061007e+00, time = 0.02s\n",
      "    Iter 1000: f = 9.283599e-01, time = 0.03s\n",
      "    Iter 1500: f = 8.222386e-01, time = 0.05s\n",
      "  Running Adam...\n",
      "    Iter 500: f = 2.472726e-02, time = 0.02s\n",
      "    Iter 1000: f = 6.327793e-03, time = 0.05s\n",
      "    Iter 1500: f = 2.671188e-03, time = 0.08s\n",
      "    Iter 2000: f = 1.404329e-03, time = 0.10s\n",
      "  Running AdaGrad...\n",
      "    Iter 0: f = 1.026279e+00, time = 0.00s\n",
      "    Iter 500: f = 2.175867e-02, time = 0.02s\n",
      "    Iter 1000: f = 9.479584e-03, time = 0.04s\n",
      "    Iter 1500: f = 5.758035e-03, time = 0.05s\n",
      "  Running OSGM-H...\n",
      "    Iter 0: f = 1.230696e+00, α = 0.10, time = 0.00s\n",
      "    Iter 500: f = 8.295067e-06, α = 49.11, time = 0.03s\n",
      "    Iter 1000: f = 3.260850e-06, α = 99.01, time = 0.06s\n",
      "    Iter 1500: f = 1.695328e-06, α = 148.94, time = 0.09s\n",
      "\n",
      "==================================================\n",
      "Problem: $\\|Ax-b\\|^8$\n",
      "==================================================\n",
      "  Running GD...\n",
      "    Iter 0: f = 1.319339e+00, time = 0.00s\n",
      "    Iter 500: f = 1.280943e+00, time = 0.02s\n",
      "    Iter 1000: f = 1.244520e+00, time = 0.03s\n",
      "    Iter 1500: f = 1.209927e+00, time = 0.05s\n",
      "  Running Adam...\n",
      "    Iter 500: f = 6.355864e-02, time = 0.02s\n",
      "    Iter 1000: f = 1.878209e-02, time = 0.04s\n",
      "    Iter 1500: f = 8.393049e-03, time = 0.06s\n",
      "    Iter 2000: f = 4.507390e-03, time = 0.08s\n",
      "  Running AdaGrad...\n",
      "    Iter 0: f = 1.169283e+00, time = 0.00s\n",
      "    Iter 500: f = 6.138159e-02, time = 0.02s\n",
      "    Iter 1000: f = 2.997674e-02, time = 0.03s\n",
      "    Iter 1500: f = 1.921857e-02, time = 0.05s\n",
      "  Running OSGM-H...\n",
      "    Iter 0: f = 1.319339e+00, α = 0.05, time = 0.00s\n",
      "    Iter 500: f = 1.985051e-06, α = 24.52, time = 0.03s\n",
      "    Iter 1000: f = 7.581795e-07, α = 49.45, time = 0.07s\n",
      "    Iter 1500: f = 4.834308e-07, α = 74.42, time = 0.10s\n",
      "\n",
      "==================================================\n",
      "Problem: Logistic (MNIST)\n",
      "==================================================\n",
      "  Running GD...\n",
      "    Iter 0: f = 5.553712e-02, time = 0.00s\n",
      "    Iter 500: f = 2.983615e-05, time = 0.87s\n",
      "    Iter 1000: f = 1.663842e-05, time = 1.67s\n",
      "    Iter 1500: f = 1.183391e-05, time = 2.43s\n",
      "  Running Adam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/9b_qh0z13pv__zzfxvd58x_r0000gn/T/ipykernel_6524/1487973385.py:125: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.where(z >= 0, np.log1p(np.exp(-z)), -z + np.log1p(np.exp(z)))\n",
      "/var/folders/3j/9b_qh0z13pv__zzfxvd58x_r0000gn/T/ipykernel_6524/1487973385.py:121: RuntimeWarning: overflow encountered in exp\n",
      "  return np.where(z >= 0, 1/(1+np.exp(-z)), np.exp(z)/(1+np.exp(z)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter 500: f = 7.323055e-09, time = 0.80s\n",
      "    Iter 1000: f = 2.502926e-09, time = 1.57s\n",
      "    Iter 1500: f = 1.265877e-09, time = 2.34s\n",
      "    Iter 2000: f = 7.530311e-10, time = 3.11s\n",
      "  Running AdaGrad...\n",
      "    Iter 0: f = 9.201191e-01, time = 0.00s\n",
      "    Iter 500: f = 8.879922e-07, time = 0.79s\n",
      "    Iter 1000: f = 4.426741e-07, time = 1.58s\n",
      "    Iter 1500: f = 2.955970e-07, time = 2.36s\n",
      "  Running OSGM-H...\n",
      "    Iter 0: f = 5.553712e-02, α = 0.87, time = 0.00s\n",
      "    Iter 500: f = 4.687950e-09, α = 25700.56, time = 1.51s\n",
      "    Iter 1000: f = 1.559815e-09, α = 42204.48, time = 3.02s\n",
      "    Iter 1500: f = 8.530036e-10, α = 48593.40, time = 4.54s\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS TABLE (after 2000 iterations)\n",
      "======================================================================\n",
      "\n",
      "Algorithm   |Ax-b|^2          |Ax-b|^4          |Ax-b|^6          |Ax-b|^8          Logistic (MNIST)  \n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "Optimality Gap:\n",
      "GD          5.74e-03          2.16e-02          7.36e-01          1.18e+00          9.28e-06          \n",
      "Adam        1.18e-07          3.67e-04          1.40e-03          4.51e-03          7.53e-10          \n",
      "AdaGrad     1.54e-04          5.47e-04          4.04e-03          1.39e-02          2.22e-07          \n",
      "OSGM-H      3.28e-04          2.43e-06          1.04e-06          3.59e-07          5.80e-10          \n",
      "\n",
      "Running Time (seconds):\n",
      "GD          0.024             0.027             0.062             0.064             3.237             \n",
      "Adam        0.040             0.047             0.101             0.077             3.112             \n",
      "AdaGrad     0.029             0.032             0.072             0.064             3.140             \n",
      "OSGM-H      0.042             0.050             0.121             0.128             6.057             \n",
      "\n",
      "--------------------------------------------------\n",
      "Logistic Regression (MNIST) Training Accuracy:\n",
      "--------------------------------------------------\n",
      "  GD: 100.00% (time: 3.237s)\n",
      "  Adam: 100.00% (time: 3.112s)\n",
      "  AdaGrad: 100.00% (time: 3.140s)\n",
      "  OSGM-H: 100.00% (time: 6.057s)\n",
      "\n",
      "======================================================================\n",
      "TIMING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Problem              GD           Adam         AdaGrad      OSGM-H      \n",
      "--------------------------------------------------------------------\n",
      "|Ax-b|^2            0.024       0.040       0.029       0.042       \n",
      "|Ax-b|^4            0.027       0.047       0.032       0.050       \n",
      "|Ax-b|^6            0.062       0.101       0.072       0.121       \n",
      "|Ax-b|^8            0.064       0.077       0.064       0.128       \n",
      "Logistic (MNIST)    3.237       3.112       3.140       6.057       \n",
      "\n",
      "Note: HDM requires 2 gradient evaluations per iteration (vs 1 for others)\n",
      "\n",
      "======================================================================\n",
      "Plots saved:\n",
      "  - hdm_comparison_iter.png  (convergence by iteration)\n",
      "  - hdm_comparison_time.png  (convergence by wall-clock time)\n",
      "  - hdm_quadratic_detailed.png\n",
      "  - hdm_quartic_detailed.png\n",
      "  - hdm_lp6_detailed.png\n",
      "  - hdm_lp8_detailed.png\n",
      "  - hdm_logistic_detailed.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HDM Experiments - Comparison with First-Order Methods\n",
    "\n",
    "Comparing:\n",
    "- GD: Gradient Descent\n",
    "- Adam\n",
    "- AdaGrad\n",
    "- HDM: Hypergradient Descent Method\n",
    "\n",
    "Test problems:\n",
    "1. Quadratic Loss: ||Ax - b||^2\n",
    "2. Quartic Loss: ||Ax - b||^4\n",
    "3. Lp Loss: ||Ax - b||^p for p=2，4,6,8\n",
    "4. Logistic Regression (unregularized) on MNIST\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "class QuadraticLoss:\n",
    "    \"\"\"f(x) = ||Ax - b||^2\"\"\"\n",
    "    def __init__(self, A, b):\n",
    "        self.A, self.b = A, b\n",
    "        self.m, self.n = A.shape\n",
    "        self.name = \"Quadratic $\\\\|Ax-b\\\\|^2$\"\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        r = self.A @ x - self.b\n",
    "        return np.linalg.norm(r)**2\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return 2 * self.A.T @ (self.A @ x - self.b)\n",
    "    \n",
    "    def optimal_value(self):\n",
    "        x_opt, _, _, _ = np.linalg.lstsq(self.A, self.b, rcond=None)\n",
    "        return self(x_opt)\n",
    "\n",
    "\n",
    "class QuarticLoss:\n",
    "    \"\"\"f(x) = ||Ax - b||^4\"\"\"\n",
    "    def __init__(self, A, b):\n",
    "        self.A, self.b = A, b\n",
    "        self.m, self.n = A.shape\n",
    "        self.name = \"Quartic $\\\\|Ax-b\\\\|^4$\"\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        r = self.A @ x - self.b\n",
    "        return np.linalg.norm(r)**4\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        r = self.A @ x - self.b\n",
    "        return 4 * np.linalg.norm(r)**2 * (self.A.T @ r)\n",
    "    \n",
    "    def optimal_value(self):\n",
    "        x_opt, _, _, _ = np.linalg.lstsq(self.A, self.b, rcond=None)\n",
    "        return self(x_opt)\n",
    "\n",
    "\n",
    "class LpLoss:\n",
    "    r\"\"\"f(x) = ||Ax - b||_2^p\"\"\"\n",
    "    def __init__(self, A, b, p):\n",
    "        self.A, self.b, self.p = A, b, float(p)\n",
    "        self.m, self.n = A.shape\n",
    "        self.name = rf\"$\\|Ax-b\\|^{int(p)}$\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        r = self.A @ x - self.b\n",
    "        return np.linalg.norm(r) ** self.p\n",
    "\n",
    "    def gradient(self, x):\n",
    "        r = self.A @ x - self.b\n",
    "        nr = np.linalg.norm(r)\n",
    "        if nr == 0.0:\n",
    "            return np.zeros_like(x)\n",
    "\n",
    "        # p = 1: grad ||r||_2 = A^T(r / ||r||_2)\n",
    "        if np.isclose(self.p, 1.0):\n",
    "            return self.A.T @ (r / nr)\n",
    "\n",
    "        # general p > 1: grad = p * ||r||^{p-2} * A^T r\n",
    "        return self.p * (nr ** (self.p - 2.0)) * (self.A.T @ r)\n",
    "\n",
    "    def optimal_value(self):\n",
    "        # Use least-squares solution as a common reference point\n",
    "        x_opt, _, _, _ = np.linalg.lstsq(self.A, self.b, rcond=None)\n",
    "        return self(x_opt)\n",
    "\n",
    "\n",
    "class SelfBoundedLoss:\n",
    "    \"\"\"f(x,y) = e^(-x) + |y|^3\"\"\"\n",
    "    def __init__(self):\n",
    "        self.n = 2\n",
    "        self.name = \"Self-Bounded $e^{-x}+|y|^3$\"\n",
    "        \n",
    "    def __call__(self, z):\n",
    "        x, y = z[0], z[1]\n",
    "        return np.exp(-x) + np.abs(y)**3\n",
    "    \n",
    "    def gradient(self, z):\n",
    "        x, y = z[0], z[1]\n",
    "        grad_y = 3 * y * np.abs(y) if y != 0 else 0.0\n",
    "        return np.array([-np.exp(-x), grad_y])\n",
    "    \n",
    "    def optimal_value(self):\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class LogisticLoss:\n",
    "    \"\"\"Unregularized logistic regression\"\"\"\n",
    "    def __init__(self, A, y, name=\"Logistic (MNIST)\"):\n",
    "        self.A, self.y = A, y\n",
    "        self.m, self.n = A.shape\n",
    "        self.name = name\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return np.where(z >= 0, 1/(1+np.exp(-z)), np.exp(z)/(1+np.exp(z)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        z = self.y * (self.A @ x)\n",
    "        loss = np.where(z >= 0, np.log1p(np.exp(-z)), -z + np.log1p(np.exp(z)))\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        z = self.y * (self.A @ x)\n",
    "        sig = self.sigmoid(-z)\n",
    "        return -(1/self.m) * self.A.T @ (self.y * sig)\n",
    "    \n",
    "    def optimal_value(self):\n",
    "        return 0.0\n",
    "    \n",
    "    def accuracy(self, x):\n",
    "        pred = np.sign(self.A @ x)\n",
    "        return np.mean(pred == self.y)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZERS\n",
    "# =============================================================================\n",
    "\n",
    "class GD:\n",
    "    \"\"\"Gradient Descent\"\"\"\n",
    "    def __init__(self, f, grad_f, alpha=0.01):\n",
    "        self.f, self.grad_f, self.alpha = f, grad_f, alpha\n",
    "        self.name = \"GD\"\n",
    "        \n",
    "    def optimize(self, x0, max_iter=2000):\n",
    "        x = x0.copy()\n",
    "        f_hist, g_hist, time_hist = [self.f(x)], [], [0.0]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for k in range(max_iter):\n",
    "            g = self.grad_f(x)\n",
    "            g_hist.append(np.linalg.norm(g))\n",
    "            x = x - self.alpha * g\n",
    "            f_hist.append(self.f(x))\n",
    "            time_hist.append(time.time() - start_time)\n",
    "            \n",
    "            if k % 500 == 0:\n",
    "                print(f\"    Iter {k}: f = {f_hist[-1]:.6e}, time = {time_hist[-1]:.2f}s\")\n",
    "        \n",
    "        self.x_final = x\n",
    "        self.total_time = time.time() - start_time\n",
    "        return f_hist, g_hist, time_hist\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Adam optimizer\"\"\"\n",
    "    def __init__(self, f, grad_f, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.f, self.grad_f = f, grad_f\n",
    "        self.alpha, self.beta1, self.beta2, self.eps = alpha, beta1, beta2, eps\n",
    "        self.name = \"Adam\"\n",
    "        \n",
    "    def optimize(self, x0, max_iter=2000):\n",
    "        x = x0.copy()\n",
    "        m, v = np.zeros_like(x), np.zeros_like(x)\n",
    "        f_hist, g_hist, time_hist = [self.f(x)], [], [0.0]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for t in range(1, max_iter + 1):\n",
    "            g = self.grad_f(x)\n",
    "            g_hist.append(np.linalg.norm(g))\n",
    "            m = self.beta1 * m + (1 - self.beta1) * g\n",
    "            v = self.beta2 * v + (1 - self.beta2) * g**2\n",
    "            m_hat = m / (1 - self.beta1**t)\n",
    "            v_hat = v / (1 - self.beta2**t)\n",
    "            x = x - self.alpha * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "            f_hist.append(self.f(x))\n",
    "            time_hist.append(time.time() - start_time)\n",
    "            \n",
    "            if t % 500 == 0:\n",
    "                print(f\"    Iter {t}: f = {f_hist[-1]:.6e}, time = {time_hist[-1]:.2f}s\")\n",
    "        \n",
    "        self.x_final = x\n",
    "        self.total_time = time.time() - start_time\n",
    "        return f_hist, g_hist, time_hist\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"AdaGrad optimizer\"\"\"\n",
    "    def __init__(self, f, grad_f, alpha=0.1, eps=1e-8):\n",
    "        self.f, self.grad_f = f, grad_f\n",
    "        self.alpha, self.eps = alpha, eps\n",
    "        self.name = \"AdaGrad\"\n",
    "        \n",
    "    def optimize(self, x0, max_iter=2000):\n",
    "        x = x0.copy()\n",
    "        G = np.zeros_like(x)\n",
    "        f_hist, g_hist, time_hist = [self.f(x)], [], [0.0]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for k in range(max_iter):\n",
    "            g = self.grad_f(x)\n",
    "            g_hist.append(np.linalg.norm(g))\n",
    "            G += g**2\n",
    "            x = x - self.alpha * g / (np.sqrt(G) + self.eps)\n",
    "            f_hist.append(self.f(x))\n",
    "            time_hist.append(time.time() - start_time)\n",
    "            \n",
    "            if k % 500 == 0:\n",
    "                print(f\"    Iter {k}: f = {f_hist[-1]:.6e}, time = {time_hist[-1]:.2f}s\")\n",
    "        \n",
    "        self.x_final = x\n",
    "        self.total_time = time.time() - start_time\n",
    "        return f_hist, g_hist, time_hist\n",
    "\n",
    "\n",
    "class HDM:\n",
    "    \"\"\"Hypergradient Descent Method with scalar stepsize (no upper bound)\"\"\"\n",
    "    def __init__(self, f, grad_f, eta=0.1, alpha_min=1e-12):\n",
    "        self.f, self.grad_f = f, grad_f\n",
    "        self.eta, self.alpha_min = eta, alpha_min\n",
    "        self.name = \"OSGM-H\"\n",
    "        \n",
    "    def optimize(self, x0, max_iter=2000, alpha0=0.01):\n",
    "        x = x0.copy()\n",
    "        alpha = alpha0\n",
    "        f_hist, g_hist, alpha_hist, time_hist = [self.f(x)], [], [alpha], [0.0]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for k in range(max_iter):\n",
    "            g = self.grad_f(x)\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            g_hist.append(g_norm)\n",
    "            \n",
    "            if g_norm < 1e-16:\n",
    "                f_hist.append(f_hist[-1])\n",
    "                alpha_hist.append(alpha)\n",
    "                time_hist.append(time.time() - start_time)\n",
    "                continue\n",
    "            \n",
    "            x_cand = x - alpha * g\n",
    "            f_cand = self.f(x_cand)\n",
    "            \n",
    "            # Null step\n",
    "            if f_cand < f_hist[-1]:\n",
    "                x_new = x_cand\n",
    "            else:\n",
    "                x_new = x.copy()\n",
    "            \n",
    "            # Hypergradient update\n",
    "            g_cand = self.grad_f(x_cand)\n",
    "            hyperg = -np.dot(g_cand, g) / (g_norm**2 + 1e-16)\n",
    "            alpha = max(alpha - self.eta * hyperg, self.alpha_min)\n",
    "            \n",
    "            x = x_new\n",
    "            f_hist.append(self.f(x))\n",
    "            alpha_hist.append(alpha)\n",
    "            time_hist.append(time.time() - start_time)\n",
    "            \n",
    "            if k % 500 == 0:\n",
    "                print(f\"    Iter {k}: f = {f_hist[-1]:.6e}, α = {alpha:.2f}, time = {time_hist[-1]:.2f}s\")\n",
    "        \n",
    "        self.alpha_hist = alpha_hist\n",
    "        self.x_final = x\n",
    "        self.total_time = time.time() - start_time\n",
    "        return f_hist, g_hist, time_hist\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def create_regression_problem(m, n, cond, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    U, _ = np.linalg.qr(np.random.randn(m, m))\n",
    "    V, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "    s = np.logspace(0, -np.log10(cond), min(m, n))\n",
    "    S = np.zeros((m, n))\n",
    "    S[:min(m,n), :min(m,n)] = np.diag(s)\n",
    "    A = U @ S @ V.T\n",
    "    x_true = np.random.randn(n)\n",
    "    b = A @ x_true + 0.01 * np.random.randn(m)\n",
    "    return A, b\n",
    "\n",
    "\n",
    "def load_mnist_data(n_samples=2000):\n",
    "    \"\"\"\n",
    "    Load MNIST dataset. \n",
    "    sklearn caches the dataset after first download, so subsequent runs are fast.\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "    X_all, y_all = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Binary classification: 0 vs 1\n",
    "    mask = (y_all == 0) | (y_all == 1)\n",
    "    X = X_all[mask]\n",
    "    y = y_all[mask]\n",
    "    y = 2 * y - 1  # Convert to {-1, +1}\n",
    "    \n",
    "    print(f\"  Full binary subset (0 vs 1): {len(y)} samples\")\n",
    "    \n",
    "    # Subsample for faster experiments\n",
    "    if n_samples is not None and n_samples < len(y):\n",
    "        np.random.seed(42)\n",
    "        idx = np.random.choice(len(y), n_samples, replace=False)\n",
    "        X, y = X[idx], y[idx]\n",
    "        print(f\"  Using {n_samples} samples\")\n",
    "    \n",
    "    X = X / 255.0\n",
    "    A = StandardScaler().fit_transform(X)\n",
    "    print(f\"  Features: {A.shape[1]} (28x28 pixels)\")\n",
    "    print(f\"  Class distribution: {np.sum(y==-1)} zeros, {np.sum(y==1)} ones\")\n",
    "    return A, y\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OSGM-H vs GD, Adam, AdaGrad (with MNIST)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Problem setup\n",
    "    m, n = 50, 20\n",
    "    cond = 100\n",
    "    max_iter = 2000\n",
    "    \n",
    "    A_reg, b_reg = create_regression_problem(m, n, cond)\n",
    "    \n",
    "    # Load MNIST\n",
    "    A_log, y_log = load_mnist_data(n_samples=2000)  # Change to None for full dataset\n",
    "    \n",
    "    # Define problems with tuned hyperparameters\n",
    "    problems = {\n",
    "        'quadratic': {\n",
    "            'loss': QuadraticLoss(A_reg, b_reg),\n",
    "            'x0': np.zeros(n),\n",
    "            'max_iter': max_iter,\n",
    "            'params': {\n",
    "                'GD': {'alpha': 0.1},\n",
    "                'Adam': {'alpha': 0.1},\n",
    "                'AdaGrad': {'alpha': 0.5},\n",
    "                'OSGM-H': {'eta': 0.5, 'alpha0': 0.01}\n",
    "            }\n",
    "        },\n",
    "        'quartic': {\n",
    "            'loss': QuarticLoss(A_reg, b_reg),\n",
    "            'x0': np.zeros(n),\n",
    "            'max_iter': max_iter,\n",
    "            'params': {\n",
    "                'GD': {'alpha': 0.001},\n",
    "                'Adam': {'alpha': 0.01},\n",
    "                'AdaGrad': {'alpha': 0.1},\n",
    "                'OSGM-H': {'eta': 0.1, 'alpha0': 0.0001}\n",
    "            }\n",
    "        },\n",
    "        # 'lp1': {\n",
    "        #     'loss': LpLoss(A_reg, b_reg, p=1.0),\n",
    "        #     'x0': np.zeros(n),\n",
    "        #     'max_iter': max_iter,\n",
    "        #     'params': {\n",
    "        #         'GD': {'alpha': 0.05},\n",
    "        #         'Adam': {'alpha': 0.01},\n",
    "        #         'AdaGrad': {'alpha': 0.2},\n",
    "        #         'HDM': {'eta': 0.5, 'alpha0': 0.01},\n",
    "        #     }\n",
    "        # },\n",
    "        'lp6': {\n",
    "            'loss': LpLoss(A_reg, b_reg, p=6.0),\n",
    "            'x0': np.zeros(n),\n",
    "            'max_iter': max_iter,\n",
    "            'params': {\n",
    "                'GD': {'alpha': 1e-5},\n",
    "                'Adam': {'alpha': 1e-3},\n",
    "                'AdaGrad': {'alpha': 1e-2},\n",
    "                'OSGM-H': {'eta': 0.1, 'alpha0': 1e-5},\n",
    "            }\n",
    "        },\n",
    "        'lp8': {\n",
    "            'loss': LpLoss(A_reg, b_reg, p=8.0),\n",
    "            'x0': np.zeros(n),\n",
    "            'max_iter': max_iter,\n",
    "            'params': {\n",
    "                'GD': {'alpha': 1e-6},\n",
    "                'Adam': {'alpha': 5e-4},\n",
    "                'AdaGrad': {'alpha': 5e-3},\n",
    "                'OSGM-H': {'eta': 0.05, 'alpha0': 1e-6},\n",
    "            }\n",
    "        },\n",
    "        # 'selfbounded': {\n",
    "        #     'loss': SelfBoundedLoss(),\n",
    "        #     'x0': np.array([0.0, 1.0]),\n",
    "        #     'max_iter': max_iter,\n",
    "        #     'params': {\n",
    "        #         'GD': {'alpha': 0.3},\n",
    "        #         'Adam': {'alpha': 0.3},\n",
    "        #         'AdaGrad': {'alpha': 1.0},\n",
    "        #         'HDM': {'eta': 0.5, 'alpha0': 0.1}\n",
    "        #     }\n",
    "        # },\n",
    "        'logistic': {\n",
    "            'loss': LogisticLoss(A_log, y_log, name=\"Logistic (MNIST)\"),\n",
    "            'x0': np.zeros(A_log.shape[1]),\n",
    "            'max_iter': max_iter,\n",
    "            'params': {\n",
    "                'GD': {'alpha': 1.0},\n",
    "                'Adam': {'alpha': 0.5},\n",
    "                'AdaGrad': {'alpha': 2.0},\n",
    "                'OSGM-H': {'eta': 55.0, 'alpha0': 1.0}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for prob_name, config in problems.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Problem: {config['loss'].name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        loss = config['loss']\n",
    "        x0 = config['x0']\n",
    "        params = config['params']\n",
    "        opt_val = loss.optimal_value()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # GD\n",
    "        print(\"  Running GD...\")\n",
    "        gd = GD(loss, loss.gradient, alpha=params['GD']['alpha'])\n",
    "        f_hist, g_hist, time_hist = gd.optimize(x0.copy(), max_iter=config['max_iter'])\n",
    "        results['GD'] = {'f_hist': f_hist, 'g_hist': g_hist, 'time_hist': time_hist,\n",
    "                         'f_gap': np.maximum(np.array(f_hist) - opt_val, 1e-16),\n",
    "                         'x_final': gd.x_final, 'total_time': gd.total_time}\n",
    "        \n",
    "        # Adam\n",
    "        print(\"  Running Adam...\")\n",
    "        adam = Adam(loss, loss.gradient, alpha=params['Adam']['alpha'])\n",
    "        f_hist, g_hist, time_hist = adam.optimize(x0.copy(), max_iter=config['max_iter'])\n",
    "        results['Adam'] = {'f_hist': f_hist, 'g_hist': g_hist, 'time_hist': time_hist,\n",
    "                           'f_gap': np.maximum(np.array(f_hist) - opt_val, 1e-16),\n",
    "                           'x_final': adam.x_final, 'total_time': adam.total_time}\n",
    "        \n",
    "        # AdaGrad\n",
    "        print(\"  Running AdaGrad...\")\n",
    "        adagrad = AdaGrad(loss, loss.gradient, alpha=params['AdaGrad']['alpha'])\n",
    "        f_hist, g_hist, time_hist = adagrad.optimize(x0.copy(), max_iter=config['max_iter'])\n",
    "        results['AdaGrad'] = {'f_hist': f_hist, 'g_hist': g_hist, 'time_hist': time_hist,\n",
    "                              'f_gap': np.maximum(np.array(f_hist) - opt_val, 1e-16),\n",
    "                              'x_final': adagrad.x_final, 'total_time': adagrad.total_time}\n",
    "        \n",
    "        # HDM\n",
    "        print(\"  Running OSGM-H...\")\n",
    "        hdm = HDM(loss, loss.gradient, eta=params['OSGM-H']['eta'])\n",
    "        f_hist, g_hist, time_hist = hdm.optimize(x0.copy(), max_iter=config['max_iter'], \n",
    "                                                  alpha0=params['OSGM-H']['alpha0'])\n",
    "        results['OSGM-H'] = {'f_hist': f_hist, 'g_hist': g_hist, 'time_hist': time_hist,\n",
    "                          'f_gap': np.maximum(np.array(f_hist) - opt_val, 1e-16),\n",
    "                          'alpha_hist': hdm.alpha_hist,\n",
    "                          'x_final': hdm.x_final, 'total_time': hdm.total_time}\n",
    "        \n",
    "        all_results[prob_name] = {'results': results, 'config': config}\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # PLOTTING\n",
    "    # ==========================================================================\n",
    "    \n",
    "    colors = {\n",
    "        'GD': '#1f77b4',\n",
    "        'Adam': '#d62728',\n",
    "        'AdaGrad': '#9467bd',\n",
    "        'OSGM-H': '#2ca02c',\n",
    "    }\n",
    "    \n",
    "    # Order and titles for all problems\n",
    "    prob_order = ['quadratic', 'quartic', 'lp6', 'lp8', 'logistic']\n",
    "    titles = [\n",
    "        '$\\\\|Ax-b\\\\|^2$',\n",
    "        '$\\\\|Ax-b\\\\|^4$',\n",
    "        '$\\\\|Ax-b\\\\|^6$',\n",
    "        '$\\\\|Ax-b\\\\|^8$',\n",
    "        'Logistic (MNIST)',\n",
    "    ]\n",
    "    n_probs = len(prob_order)\n",
    "    \n",
    "    # Main comparison figure (2 x n_probs) - by iteration\n",
    "    fig, axes = plt.subplots(2, n_probs, figsize=(4 * n_probs, 8))\n",
    "    \n",
    "    for col, prob_name in enumerate(prob_order):\n",
    "        title = titles[col]\n",
    "        results = all_results[prob_name]['results']\n",
    "        max_iter_prob = all_results[prob_name]['config']['max_iter']\n",
    "        \n",
    "        # Function value gap (top row)\n",
    "        ax = axes[0, col]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            ax.semilogy(res['f_gap'], label=name, color=colors[name], \n",
    "                       linewidth=2, alpha=0.9)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Function value gap')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, max_iter_prob])\n",
    "        \n",
    "        # Gradient norm (bottom row)\n",
    "        ax = axes[1, col]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            ax.semilogy(res['g_hist'], label=name, color=colors[name],\n",
    "                       linewidth=2, alpha=0.9)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Gradient Norm')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, max_iter_prob])\n",
    "    \n",
    "    # Legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=4, fontsize=11,\n",
    "               bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    plt.suptitle('OSGM-H vs First-Order Methods (by Iteration)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig('hdm_comparison_iter.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Comparison by wall-clock time (2 x n_probs)\n",
    "    fig, axes = plt.subplots(2, n_probs, figsize=(4 * n_probs, 8))\n",
    "    \n",
    "    for col, prob_name in enumerate(prob_order):\n",
    "        title = titles[col]\n",
    "        results = all_results[prob_name]['results']\n",
    "        \n",
    "        # Function value gap vs time (top row)\n",
    "        ax = axes[0, col]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            ax.semilogy(res['time_hist'], res['f_gap'], label=name, color=colors[name], \n",
    "                       linewidth=2, alpha=0.9)\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Function value gap')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norm vs time (bottom row)\n",
    "        ax = axes[1, col]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            # time_hist has one more element than g_hist\n",
    "            ax.semilogy(res['time_hist'][1:], res['g_hist'], label=name, color=colors[name],\n",
    "                       linewidth=2, alpha=0.9)\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Gradient Norm')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=4, fontsize=11,\n",
    "               bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    plt.suptitle('OSGM-H vs First-Order Methods (by Wall-Clock Time)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "    plt.savefig('hdm_comparison_time.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Results Table\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"FINAL RESULTS TABLE (after {max_iter} iterations)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n{'Algorithm':<12}\", end=\"\")\n",
    "    for title in titles:\n",
    "        clean_title = title.replace('$', '').replace('\\\\|', '|').replace('\\\\', '')\n",
    "        if len(clean_title) > 16:\n",
    "            clean_title = clean_title[:14] + \"..\"\n",
    "        print(f\"{clean_title:<18}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 84)\n",
    "    \n",
    "    print(\"\\nOptimality Gap:\")\n",
    "    for method in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "        print(f\"{method:<12}\", end=\"\")\n",
    "        for prob_name in prob_order:\n",
    "            results = all_results[prob_name]['results']\n",
    "            final_gap = results[method]['f_gap'][-1]\n",
    "            print(f\"{final_gap:<18.2e}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nRunning Time (seconds):\")\n",
    "    for method in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "        print(f\"{method:<12}\", end=\"\")\n",
    "        for prob_name in prob_order:\n",
    "            results = all_results[prob_name]['results']\n",
    "            total_time = results[method]['total_time']\n",
    "            print(f\"{total_time:<18.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Print logistic regression accuracy\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Logistic Regression (MNIST) Training Accuracy:\")\n",
    "    print(\"-\" * 50)\n",
    "    loss_log = all_results['logistic']['config']['loss']\n",
    "    for method in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "        x_final = all_results['logistic']['results'][method]['x_final']\n",
    "        acc = loss_log.accuracy(x_final) * 100\n",
    "        total_time = all_results['logistic']['results'][method]['total_time']\n",
    "        print(f\"  {method}: {acc:.2f}% (time: {total_time:.3f}s)\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Individual detailed plots with stepsize\n",
    "    # ==========================================================================\n",
    "    \n",
    "    for prob_name, title in zip(prob_order, titles):\n",
    "        results = all_results[prob_name]['results']\n",
    "        config = all_results[prob_name]['config']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "        \n",
    "        # Function value gap\n",
    "        ax = axes[0]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            ax.semilogy(res['f_gap'], label=name, color=colors[name], linewidth=2)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Function value gap')\n",
    "        ax.set_title('Function Value Gap')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norm\n",
    "        ax = axes[1]\n",
    "        for name in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            res = results[name]\n",
    "            ax.semilogy(res['g_hist'], label=name, color=colors[name], linewidth=2)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Gradient Norm')\n",
    "        ax.set_title('Gradient Norm')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # HDM stepsize\n",
    "        ax = axes[2]\n",
    "        ax.semilogy(results['OSGM-H']['alpha_hist'], color=colors['OSGM-H'], linewidth=2)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Stepsize $\\\\alpha$')\n",
    "        ax.set_title('OSGM-H Learned Stepsize')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'{title}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'hdm_{prob_name}_detailed.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Summary timing table\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TIMING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n{'Problem':<20} {'GD':<12} {'Adam':<12} {'AdaGrad':<12} {'OSGM-H':<12}\")\n",
    "    print(\"-\" * 68)\n",
    "    for prob_name, title in zip(prob_order, titles):\n",
    "        results = all_results[prob_name]['results']\n",
    "        clean_title = title.replace('$', '').replace('\\\\|', '|').replace('\\\\', '')[:18]\n",
    "        print(f\"{clean_title:<20}\", end=\"\")\n",
    "        for method in ['GD', 'Adam', 'AdaGrad', 'OSGM-H']:\n",
    "            t = results[method]['total_time']\n",
    "            print(f\"{t:<12.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nNote: HDM requires 2 gradient evaluations per iteration (vs 1 for others)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Plots saved:\")\n",
    "    print(\"  - hdm_comparison_iter.png  (convergence by iteration)\")\n",
    "    print(\"  - hdm_comparison_time.png  (convergence by wall-clock time)\")\n",
    "    for prob_name in prob_order:\n",
    "        print(f\"  - hdm_{prob_name}_detailed.png\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
